{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip show transformers","metadata":{"execution":{"iopub.status.busy":"2024-05-29T12:04:34.767106Z","iopub.execute_input":"2024-05-29T12:04:34.767470Z","iopub.status.idle":"2024-05-29T12:04:46.921605Z","shell.execute_reply.started":"2024-05-29T12:04:34.767438Z","shell.execute_reply":"2024-05-29T12:04:46.920530Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Name: transformers\nVersion: 4.39.3\nSummary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\nHome-page: https://github.com/huggingface/transformers\nAuthor: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\nAuthor-email: transformers@huggingface.co\nLicense: Apache 2.0 License\nLocation: /opt/conda/lib/python3.10/site-packages\nRequires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\nRequired-by: kaggle-environments\n","output_type":"stream"}]},{"cell_type":"code","source":"#!pip install llama-cpp-haystack\n!export LLAMA_CUBLAS=1\n!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python","metadata":{"execution":{"iopub.status.busy":"2024-05-29T12:10:16.779493Z","iopub.execute_input":"2024-05-29T12:10:16.779929Z","iopub.status.idle":"2024-05-29T12:10:29.870412Z","shell.execute_reply.started":"2024-05-29T12:10:16.779893Z","shell.execute_reply":"2024-05-29T12:10:29.869180Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: llama-cpp-python in /opt/conda/lib/python3.10/site-packages (0.2.76)\nRequirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from llama-cpp-python) (4.9.0)\nRequirement already satisfied: numpy>=1.20.0 in /opt/conda/lib/python3.10/site-packages (from llama-cpp-python) (1.26.4)\nRequirement already satisfied: diskcache>=5.6.1 in /opt/conda/lib/python3.10/site-packages (from llama-cpp-python) (5.6.3)\nRequirement already satisfied: jinja2>=2.11.3 in /opt/conda/lib/python3.10/site-packages (from llama-cpp-python) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.3)\n","output_type":"stream"}]},{"cell_type":"code","source":"from haystack_integrations.components.generators.llama_cpp import LlamaCppGenerator","metadata":{"execution":{"iopub.status.busy":"2024-05-29T12:12:44.898093Z","iopub.execute_input":"2024-05-29T12:12:44.898474Z","iopub.status.idle":"2024-05-29T12:12:51.510081Z","shell.execute_reply.started":"2024-05-29T12:12:44.898444Z","shell.execute_reply":"2024-05-29T12:12:51.509127Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"!wget https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q2_K.gguf?download=true","metadata":{"execution":{"iopub.status.busy":"2024-05-29T12:13:10.632061Z","iopub.execute_input":"2024-05-29T12:13:10.633534Z","iopub.status.idle":"2024-05-29T12:15:13.058604Z","shell.execute_reply.started":"2024-05-29T12:13:10.633497Z","shell.execute_reply":"2024-05-29T12:15:13.057445Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"--2024-05-29 12:13:11--  https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q2_K.gguf?download=true\nResolving huggingface.co (huggingface.co)... 13.35.7.38, 13.35.7.81, 13.35.7.57, ...\nConnecting to huggingface.co (huggingface.co)|13.35.7.38|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://cdn-lfs-us-1.huggingface.co/repos/72/62/726219e98582d16c24a66629a4dec1b0761b91c918e15dea2625b4293c134a92/6d52a37032d25bc879691b0e870da12ffd5788e6f4dd2eea1b5d9a7020091f71?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27mistral-7b-instruct-v0.2.Q2_K.gguf%3B+filename%3D%22mistral-7b-instruct-v0.2.Q2_K.gguf%22%3B&Expires=1717243991&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxNzI0Mzk5MX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzcyLzYyLzcyNjIxOWU5ODU4MmQxNmMyNGE2NjYyOWE0ZGVjMWIwNzYxYjkxYzkxOGUxNWRlYTI2MjViNDI5M2MxMzRhOTIvNmQ1MmEzNzAzMmQyNWJjODc5NjkxYjBlODcwZGExMmZmZDU3ODhlNmY0ZGQyZWVhMWI1ZDlhNzAyMDA5MWY3MT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=f2Fev4cbz1uP8380-PZAdYYhBYFGq8hn%7E7Ni8r8gBjRRGSagT5fea%7EVPGNGE-TgfYDMd5RYiH77UrKtr%7EF%7EacF%7ETVoLe8lt6aoybWxsfVP36-phKXLSmh3JL7rlSd1Aa3pMd7YhhlWPwwEM6tH1JWH6ZjkXej1XxtDNoiGO9PPYJWzwqQ0im6BjnJ4WXdamX-2797ur30Fw9fcekrWGhLE5Y3QIPH9AybJeXNErD824IR3RfWiXoSosJnT95jiKVYfN7J8z3Eoa4GCLlC6VFMVkdtVZ0z6gJdQVLCf9IqlYT3t9nWolkmmDfXhixbIWTHiyvciCELuvG2eMIvSWOwg__&Key-Pair-Id=KCD77M1F0VK2B [following]\n--2024-05-29 12:13:11--  https://cdn-lfs-us-1.huggingface.co/repos/72/62/726219e98582d16c24a66629a4dec1b0761b91c918e15dea2625b4293c134a92/6d52a37032d25bc879691b0e870da12ffd5788e6f4dd2eea1b5d9a7020091f71?response-content-disposition=attachment%3B+filename*%3DUTF-8''mistral-7b-instruct-v0.2.Q2_K.gguf%3B+filename%3D%22mistral-7b-instruct-v0.2.Q2_K.gguf%22%3B&Expires=1717243991&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxNzI0Mzk5MX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzcyLzYyLzcyNjIxOWU5ODU4MmQxNmMyNGE2NjYyOWE0ZGVjMWIwNzYxYjkxYzkxOGUxNWRlYTI2MjViNDI5M2MxMzRhOTIvNmQ1MmEzNzAzMmQyNWJjODc5NjkxYjBlODcwZGExMmZmZDU3ODhlNmY0ZGQyZWVhMWI1ZDlhNzAyMDA5MWY3MT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=f2Fev4cbz1uP8380-PZAdYYhBYFGq8hn~7Ni8r8gBjRRGSagT5fea~VPGNGE-TgfYDMd5RYiH77UrKtr~F~acF~TVoLe8lt6aoybWxsfVP36-phKXLSmh3JL7rlSd1Aa3pMd7YhhlWPwwEM6tH1JWH6ZjkXej1XxtDNoiGO9PPYJWzwqQ0im6BjnJ4WXdamX-2797ur30Fw9fcekrWGhLE5Y3QIPH9AybJeXNErD824IR3RfWiXoSosJnT95jiKVYfN7J8z3Eoa4GCLlC6VFMVkdtVZ0z6gJdQVLCf9IqlYT3t9nWolkmmDfXhixbIWTHiyvciCELuvG2eMIvSWOwg__&Key-Pair-Id=KCD77M1F0VK2B\nResolving cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)... 13.35.166.59, 13.35.166.55, 13.35.166.96, ...\nConnecting to cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)|13.35.166.59|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 3083098400 (2.9G) [binary/octet-stream]\nSaving to: 'mistral-7b-instruct-v0.2.Q2_K.gguf?download=true'\n\nmistral-7b-instruct 100%[===================>]   2.87G  24.4MB/s    in 2m 1s   \n\n2024-05-29 12:15:12 (24.4 MB/s) - 'mistral-7b-instruct-v0.2.Q2_K.gguf?download=true' saved [3083098400/3083098400]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"generator = LlamaCppGenerator(\n    model=\"/kaggle/working/mistral-7b-instruct-v0.2.Q2_K.gguf?download=true\", \n    n_ctx=512,\n    n_batch=128,\n    model_kwargs={\"n_gpu_layers\": -1},\ngeneration_kwargs={\"max_tokens\": 1200, \"temperature\": 0.1},\n)\ngenerator.warm_up()\nprompt = f\"explain the concept of deep learning\"\nresult = generator.run(prompt)\noutput = result\nfiltered_output = [{'text': output['replies'][0], 'index': 0, 'logprobs': None, 'finish_reason': 'length'}]\nfinal_answer = filtered_output[0]['text']\nfinal_answer","metadata":{"execution":{"iopub.status.busy":"2024-05-29T12:23:26.408322Z","iopub.execute_input":"2024-05-29T12:23:26.408803Z","iopub.status.idle":"2024-05-29T12:25:04.886205Z","shell.execute_reply.started":"2024-05-29T12:23:26.408774Z","shell.execute_reply":"2024-05-29T12:25:04.884471Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /kaggle/working/mistral-7b-instruct-v0.2.Q2_K.gguf?download=true (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\nllama_model_loader: - kv   2:                       llama.context_length u32              = 32768\nllama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv   4:                          llama.block_count u32              = 32\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\nllama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv  11:                          general.file_type u32              = 10\nllama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\nllama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\nllama_model_loader: - kv  23:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type q2_K:   65 tensors\nllama_model_loader: - type q3_K:  160 tensors\nllama_model_loader: - type q6_K:    1 tensors\nllm_load_vocab: special tokens definition check successful ( 259/32000 ).\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: n_ctx_train      = 32768\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 8\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 4\nllm_load_print_meta: n_embd_k_gqa     = 1024\nllm_load_print_meta: n_embd_v_gqa     = 1024\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 14336\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 1000000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_yarn_orig_ctx  = 32768\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = Q2_K - Medium\nllm_load_print_meta: model params     = 7.24 B\nllm_load_print_meta: model size       = 2.87 GiB (3.41 BPW) \nllm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: PAD token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_tensors: ggml ctx size =    0.15 MiB\nllm_load_tensors:        CPU buffer size =  2939.57 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 512\nllama_new_context_with_model: n_batch    = 128\nllama_new_context_with_model: n_ubatch   = 128\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 1000000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\nllama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\nllama_new_context_with_model:        CPU compute buffer size =    20.25 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 1\nAVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \nModel metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'llama.context_length': '32768', 'general.name': 'mistralai_mistral-7b-instruct-v0.2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '10'}\nAvailable chat formats from metadata: chat_template.default\nGuessed chat format: mistral-instruct\n\nllama_print_timings:        load time =    1483.24 ms\nllama_print_timings:      sample time =     197.99 ms /   304 runs   (    0.65 ms per token,  1535.42 tokens per second)\nllama_print_timings: prompt eval time =    1483.17 ms /     7 tokens (  211.88 ms per token,     4.72 tokens per second)\nllama_print_timings:        eval time =   95742.78 ms /   303 runs   (  315.98 ms per token,     3.16 tokens per second)\nllama_print_timings:       total time =   97844.48 ms /   310 tokens\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"'\\n\\nDeep learning is a subset of machine learning, which in turn is a subfield of artificial intelligence (AI). Deep learning models are neural networks with three or more hidden layers. These models learn to represent data by recognizing patterns and features from large amounts of input data, without being explicitly programmed for the task.\\n\\nDeep learning models can be thought of as a series of non-linear transformations that take raw data as input and output high-level abstractions or representations of the data. Each transformation in the sequence is performed by a set of interconnected nodes or neurons, which are organized into layers. The first layer typically takes the raw data as input, while the last layer outputs the final representation or prediction.\\n\\nThe key innovation behind deep learning is the use of activation functions and backpropagation to enable the model to learn complex representations from large amounts of data. Activation functions introduce non-linearity into the model, allowing it to learn non-linear relationships between inputs and outputs. Backpropagation is an algorithm used to train neural networks by calculating the gradient of the loss function with respect to each weight in the network and adjusting the weights accordingly.\\n\\nDeep learning models have achieved state-of-the-art results in various applications, including image recognition, speech recognition, natural language processing, and game playing. These models can learn complex representations from large amounts of data, enabling them to perform tasks that were previously thought to require human-level intelligence.'"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}